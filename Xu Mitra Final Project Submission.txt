{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\Omegav}{\\boldsymbol{\\Omega}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Report: Training Flappy Bird Using Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sigmon Xu, Saptashwa Mitra*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flappy Bird is a side-scrolling mobile game released in 2013. The aim of Flappy Bird is to navigate the bird through a series of green pipes. If the player passes a pair of pipes, his score increases by one point. The aim is to get as high a score as possible. Although the original Flappy Bird game is said to end at score 999, several online variations of the game have made it a never-ending one, meaning the game could go on as long as the player can stay alive. \n",
    "\n",
    "In our final project, we wish to apply Q Learning(RL) technique on the Flappy Bird bot make it learn how to play to get a high score, that is generally unachievable by a regular user.\n",
    "\n",
    "The general average score a normal player can score in this game ranges from 20-30, with the highest medal being awarded if the user can score above 40. Leaderboards on Google Play will show high scores will show high scores in the order of billions because there is a way to manually enter your score into the game. However, other sites and blogs mentioning application of RL techniques to Flappy Bird have mentioned getting high scores above 1000, the highest we observed being close to 4000. In this project, after training using the simple Q Learning technique(we have not used a Neural Network for the Q function here), our bot was able to achieve a high score of 9797."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We needed a game engine on which we could implement our RL technique. We chose to use the Flappy Bird typing tutor made available by ***@mrspeaker*** in the following link: https://github.com/mrspeaker/Omega500\n",
    "\n",
    "The game is written in JavaScript and with a few tweaks, this typing tutor game can be converted into a normal flappy bird game, where presing \"S\" makes the bird jump.\n",
    "\n",
    "Due to the nature of the game, each run at a reaching a high score in this game takes a considerable amount of time. That is why for this project, we have made 2 separate versions of the software:\n",
    "\n",
    "1) ***Training: ***The first is to be used for training the bird. Training the bird is time consuming, as mentioned before. We found that it took aroung 4 hours of training(nearly 500 games for the bird to regularly get a score of over 1000). Please note that although the screen of the game says \"Press S\", that is to externally control the bird. It should not be pressed during either the training or the testing phase. \n",
    "\n",
    "The training software logs the list of scores in the browser console, along with the current values in the Q array. We have monitored the scores being scored by the bird over time and when the scores by the bird seem to be consistently high on each run, we have used the Q array output from that instance to be used by the Testing version of the software.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "2) ***Testing: ***For testing out the model, we have a separate project named **Flappy2**. In there, once the Q array from Step 1 is loaded into the game screen, the bird will score very high points, which is way above what an average human player could score.\n",
    "\n",
    "\n",
    "Enjoy the hacking for Flappy bird!  :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main algorithm we used in this project is Q-Learning.\n",
    "\n",
    "We have decided on two parameters for training our model based on the coordinates of the current position of the bird with respect to the lowermost pipe. At any point of time, the bird can have two choices, either to jump(**\"fly\"**) or do nothing(**\"stay\"**). The Q array is thus a 135x45x2 array. A detailed description of the parameters is mentioned below.\n",
    "\n",
    "The formula used is the simple formula for Q-Learning:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      Q(s_t,a_t) = Q(s_t,a_t) + \\rho (r_{t+1}  - Q(s_t,a_t))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "`Step 1:` We first observe the current state and perform the action which can maximise our return value. If this is a new state we never explore before, use random move, after this the bird is in new state. \n",
    "\n",
    "`Step 2:` Observe the new state and check the return value. \n",
    "\n",
    "`Step 3:` Update Q by using above formula. \n",
    "\n",
    "`Step 4:` Update the old state to the new state and start again.\n",
    "\n",
    "Details of the implementation of this technique is described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Results/parameters.png\", width=300,height=300>\n",
    "\n",
    "***State and Actions:***\n",
    "\n",
    "**State**\n",
    "\n",
    "We have decided at any point of time the X and Y coordinate of the bird could be a good choice for determining the state of the bird. We have used a variant of the X and Y coordinates here. Because the main goal of the bird is to cruise by without avoiding the pipes, we have chosen the X and Y coordinate of the bird with respect to the lower pipe. Since the space between the two pipes is a constant, we do not need to take the position of the other pipe into consideration for the position of the bird.\n",
    "\n",
    "\n",
    "Also, since the space between the bird and a pipe is big, we have divided the while possible space between a bird and the pipe into 4x4 blocks. The position of the beak of the bird is used to determine the position of the bird on the screen and which block it falls into.\n",
    "\n",
    "Following is how the vertical and horizontal distance of the bird from the lower pipe is calculated (See the image above) :\n",
    "\n",
    "* **H:**Horizontal_distance(the horizontal distance between the bird and the lower pipe). \n",
    "\n",
    "    First find the next closest pair of pipes. Select the lower pipe. The width of a pipe is constant.\n",
    "    \n",
    "        horizontal_distance = pipe.w + pipe.h - bird.x\n",
    "\n",
    "* **V:**Vertical_distance(the vertical distance between the bird and the lower pipe).\n",
    "    \n",
    "        vertical_distance = bird.y - pipe.y\n",
    " \n",
    "**Actions**\n",
    "\n",
    "\n",
    "Since this game only have one jump function. We defined our actions as:\n",
    "\n",
    "* fly: bird performs one single jump. \n",
    "\n",
    "* stay: do nothing.\n",
    "\n",
    "**Return value:** \n",
    "\n",
    "After reading of other implementations online of this game and other continuous games we decided on rewarding the bird for every move that keeps it alive. Also, we are targetting a very high score, the bird can accumulate high rewards in a long continuous run. To offset that, we decided on making the return for a death as high. We set the following return values :\n",
    "\n",
    "* every time the bird dies, we set that to be a very large negative number, like -1500.\n",
    "\n",
    "* every time the bird passes one set of pipes, we make the return value equal to 1.\n",
    "\n",
    "`Random Move:`\n",
    "\n",
    "We use the Math.random() function to let the bird perform randomness actions initially. Then by setting up two parameters, \n",
    "\n",
    "    gamma = gamma * epsilon;\n",
    "    \n",
    "    Gamma: 0.9\n",
    "    \n",
    "    Epsilon: 1\n",
    "    \n",
    "epsilon and gamma to decrease the range of randomness after one sucessive move by checking whether Math.random() < gamma.\n",
    "\n",
    "\n",
    "**Monitoring the Training Process: **\n",
    "\n",
    "In order to determine when to stop the training the bird, we have monitored the scores generated over time. Our software prints out the Q array value and the list of scores at regular intervals into the browser console. We have monitored the improvement of the scores over time using a simple python script (\"testPy.py\") and when the bird seems to be scoring above a desired value at regular trials, we can use the latest Q array printed into the browser console for testing purposes.\n",
    "\n",
    "The following is a sample graph of the scores of the bird over time:\n",
    "\n",
    "<img src=\"Results/status.png\", width=500,height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Running and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the zipped file provided and open the **\"FlappyBirdProject\"** folder.\n",
    "\n",
    "\n",
    "***Running: ***\n",
    "\n",
    "In order to directly run the game with an already trained model, please find the file **\"Results/qinput.txt\"**. This file contains the trained Q-Array. Copy the contents of that file.\n",
    "\n",
    "Now open the project folder named **Flappy2** and open the **index.html** file using your browser. Please paste the copied contents on the textbox on the top left corner of the page and to get the bird play using the trained Q array.\n",
    "\n",
    "\n",
    "***Testing: ***\n",
    "\n",
    "In order to start training the bird, open the project folder **\"Flappy\"** and open the **index.html** file. You will find that the bird starts training once you click on the play button. In order to monitor the status of the testing, you will find that the Q-Array and the list of scores gets printed in the browser console every 100 games for the first 400 games, after which they get printed every 10 games.\n",
    "\n",
    "\n",
    "***Monitoring Scores: ***\n",
    "\n",
    "In order to monitor the scores by the bird, to determine when to stop training, please copy the output containing the scores printed in the console and replace the contents of the file **\"data.txt\"** with the new scores. Run **\"testPy.py\"** to get the graph showing the improvement of scores with each game being played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) After training, the bird routinely scores above 5000 with the highest observed score being 9797. We have attached a short video of the run in the **\"Results/demo.mp4\"** file enclosed in the project submission. The following is a snapshot from that run:\n",
    "<img src=\"Results/HS.png\", width=200,height=300>\n",
    "\n",
    "\n",
    "2) We have attached a text file( **\"Results/qinput.txt\"** ) containing a trained Q array that can be loaded into the second variant of the Flappy Bird game provided to achieve really high scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing we noticed while running the bird was that the bird tends to stick closer to the lower pipe, just clearing the height of the lower pipe in every jump. We believe it is because the state parameters for the bird are calculating using the vertical and horizontal distance of the bird with respect to the lower pipe.\n",
    "\n",
    "\n",
    "Another observation we made was that in the Q-Array, several states have return of 0(even with the randomness), implying that the bird has not visited these states. This makes sense since there are positions in the whole space where it does not make sense for the bird to be in at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dimension that could be added to the state of the bird could be the direction, by which we mean whether the bird is currently going up or down. We believe this could help in the training of the bird. This was an after-thought and we could not implement this due to shortage of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/27(Xu): Start to play the game and read the related articles.\n",
    "\n",
    "4/27(Mitra): Find the right game engine(Omega 500) for our project.\n",
    "\n",
    "4/27(Xu, Mitra): First Team discussion about the schedule and the steps to finish the project.\n",
    "\n",
    "5/1(Xu, Mitra): Look into the game engine to understand the parameters, mainly understand how the game engine works for the Flappy Bird. \n",
    "\n",
    "5/6(Mitra, Xu): Start to look at other people's implementations online for RL in Flappy Bird and try to understand.\n",
    "\n",
    "5/8(Mitra): Implement Q-Learning on top of the $\\Omega$500 game engine. \n",
    "\n",
    "5/8(Xu): Finish the final report.\n",
    "\n",
    "5/8(Mitra, Xu): Discuss about the results we get and how we can perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Idea of which parameters to use to train the model: https://github.com/SarvagyaVaish/FlappyBirdRL\n",
    "\n",
    "2) Game Engine: $\\Omega$500 Engine by @mrspeaker: https://github.com/mrspeaker/Omega500\n",
    "\n",
    "3) Algorithm for Reinforcement Learning from Notebook #22-23 from lectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
